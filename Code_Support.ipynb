{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "'''\n",
    "LABEL 0 : NON - MASKED PERSON\n",
    "LABEL 1: MASKED PERSON\n",
    "\n",
    "'''\n",
    "person_no_mask_labels = {'eyeglasses','face_no_mask','face_other_covering','hat','sunglasses','turban'}\n",
    "not_person = 'not_person'\n",
    "\n",
    "with open('Labels.csv','w',newline='') as file:\n",
    "    wr = csv.writer(file,delimiter=',')    \n",
    "    wr.writerow(['ImageNo','Label','Tags'])\n",
    "    \n",
    "\n",
    "path_to_dataset = 'D:\\pravesh\\\\Concordia\\\\2021-Winter\\\\COMP-6721-Intro_To_AI\\\\project\\\\1\\dataset\\\\'\n",
    "image_file_names= [i for i in os.listdir(path_to_dataset+'images')]\n",
    "\n",
    "rows = []\n",
    "for index, image_name  in enumerate(image_file_names):\n",
    "    path_to_json = path_to_dataset + 'annotations\\\\' + image_name + '.json'\n",
    "    data = json.load(open(path_to_json))\n",
    "    image_tags = set()\n",
    "    label = 0\n",
    "    for item in data['Annotations']:\n",
    "        image_tags.add(item['classname'])\n",
    "    rows.append((image_name, image_tags))\n",
    "\n",
    "print(len(rows))\n",
    "\n",
    "with open('Labels.csv','a',newline='') as file:\n",
    "    for image_name, image_tags in rows:\n",
    "        label = 0\n",
    "        if not_person in image_tags:\n",
    "            label = 2\n",
    "        elif len(image_tags & person_no_mask_labels)>0:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        wr = csv.writer(file,delimiter=',') \n",
    "        wr.writerow([image_name, label, ','.join(image_tags)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are 6435.jpg, 1, {'face_with_mask_incorrect', 'mask_surgical'}\n",
      "Processed 1700 lines.\n",
      "{'mask_surgical', 'face_with_mask_incorrect', 'goggles', 'hair_net', 'balaclava_ski_mask', 'eyeglasses', 'sunglasses', 'other', 'hijab_niqab', 'face_other_covering', 'hood', 'helmet', 'mask_colorful', 'face_shield', 'hat', 'turban', 'face_with_mask', 'face_no_mask', 'scarf_bandana', 'gas_mask'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "tags = set()\n",
    "\n",
    "with open('Labels.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for (name, label, image_tags) in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "#             print(f'{name}, {label}, {image_tags}')\n",
    "            tags = tags | set(image_tags.split(','))\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segregate with categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are ImageNo, Label, Tags\n",
      "Processed 3649 lines.\n",
      "3648\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "tags = {'mask_surgical', 'face_with_mask_incorrect', 'goggles', 'hair_net'\n",
    "        , 'balaclava_ski_mask', 'eyeglasses', 'sunglasses', 'other'\n",
    "        , 'hijab_niqab', 'face_other_covering', 'hood', 'helmet'\n",
    "        , 'mask_colorful', 'face_shield', 'hat', 'turban', 'face_with_mask'\n",
    "        , 'face_no_mask', 'scarf_bandana', 'gas_mask'}\n",
    "\n",
    "rows = []\n",
    "\n",
    "path_to_dataset = 'D:\\pravesh\\\\Concordia\\\\2021-Winter\\\\COMP-6721-Intro_To_AI\\\\project\\\\1\\dataset\\\\'\n",
    "\n",
    "with open('Labels.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "#             print(f'{name}, {label}, {image_tags}')\n",
    "            rows.append(row)\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "\n",
    "print(len(rows))\n",
    "\n",
    "for tag in tags:\n",
    "    path_to_tag_directory = path_to_dataset + 'images\\\\' + tag\n",
    "    if not os.path.isdir(path_to_tag_directory):\n",
    "        os.mkdir(path_to_tag_directory)\n",
    "    for (name, _, image_tags) in rows:\n",
    "        path_to_file = path_to_dataset + 'images\\\\' + name\n",
    "#         print(path_to_file)\n",
    "#         print(os.path.isfile(path_to_file))\n",
    "        if os.path.isfile(path_to_file):\n",
    "            if tag in image_tags:\n",
    "                new_file_path = path_to_tag_directory+'\\\\'+ name\n",
    "#                 print(path_to_file, new_file_path)\n",
    "                os.rename(path_to_file, new_file_path)\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask 1749 3\n",
      "not_person 450 3\n",
      "no_mask 892 3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "'''\n",
    "LABEL 0 : NON - MASKED PERSON\n",
    "LABEL 1: MASKED PERSON\n",
    "\n",
    "'''\n",
    "person_no_mask_labels = {'eyeglasses','face_no_mask','face_other_covering','hat','sunglasses','turban'}\n",
    "label_map = {'mask': 0,\n",
    "            'no_mask': 1,\n",
    "            'not_person': 2}\n",
    "    \n",
    "\n",
    "path_to_dataset = 'D:/pravesh/Concordia/2021-Winter/COMP-6721-Intro_To_AI/project/1/dataset/'\n",
    "\n",
    "images_dir = path_to_dataset+'images/'\n",
    "\n",
    "dir_names= [i for i in os.listdir(images_dir)]\n",
    "\n",
    "rows = {0: [], 1:[], 2:[]}\n",
    "for index, dir_name in enumerate(dir_names):\n",
    "    count=0\n",
    "    label = label_map[dir_name]\n",
    "    dir_path = images_dir + dir_name + '/'\n",
    "    image_names= [i for i in os.listdir(dir_path)]\n",
    "    for index, image_name  in enumerate(image_names):\n",
    "        path_to_json = path_to_dataset + 'annotations/' + image_name + '.json'\n",
    "        data = json.load(open(path_to_json))\n",
    "        image_tags = set()\n",
    "        for item in data['Annotations']:\n",
    "            image_tags.add(item['classname'])\n",
    "        rows[label].append((dir_name+'/'+image_name, label, image_tags))\n",
    "        count+=1\n",
    "    print(dir_name, count, len(rows))\n",
    "        \n",
    "print(len(rows))\n",
    "\n",
    "test_label_file = path_to_dataset+'test_label_info.csv'\n",
    "\n",
    "with open(test_label_file,'w',newline='') as file:\n",
    "    wr = csv.writer(file,delimiter=',')    \n",
    "    wr.writerow(['ImageNo','Label','Tags'])\n",
    "\n",
    "with open(test_label_file,'a',newline='') as file:\n",
    "    wr = csv.writer(file,delimiter=',')\n",
    "    for i in range(3):\n",
    "        label_rows = rows[i]\n",
    "        for j in range(100):\n",
    "            image, label, image_tags = label_rows[j]\n",
    "            wr.writerow([image, label])\n",
    "            \n",
    "train_label_file = path_to_dataset+'train_label_info.csv'\n",
    "\n",
    "with open(train_label_file,'w',newline='') as file:\n",
    "    wr = csv.writer(file,delimiter=',')    \n",
    "    wr.writerow(['ImageNo','Label','Tags'])      \n",
    "            \n",
    "with open(train_label_file,'a',newline='') as file:\n",
    "    wr = csv.writer(file,delimiter=',')\n",
    "    for i in range(3):\n",
    "        label_rows = rows[i]\n",
    "        for j in range(100, 100+350, 1):\n",
    "            image, label, image_tags = label_rows[j]\n",
    "            wr.writerow([image, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-b180bc99e072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mProjectDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Face Landmarks dataset.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_info_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class ProjectDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, debug=False):\n",
    "        self.label_info_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.debug = debug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_info_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        label_img_name = self.label_info_frame.iloc[idx, 0]\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                label_img_name)\n",
    "        image = io.imread(img_name)\n",
    "        label = self.label_info_frame.iloc[idx, 1]\n",
    "        label = np.array([label])\n",
    "        landmarks = label.astype('float').reshape(-1, 1)\n",
    "        sample = {'image': image\n",
    "                  ,'label': label\n",
    "                  ,'name': label_img_name}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.debug:\n",
    "            print(label_img_name)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int))\n",
    "        self.output_size = output_size\n",
    "        self.debug = False\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size, self.output_size * w / h\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        \n",
    "        # Clipping or filling\n",
    "        if new_w>self.output_size:\n",
    "            mid = new_w//2\n",
    "            new_w_start = mid-self.output_size//2\n",
    "            new_w_end = mid+self.output_size//2\n",
    "            \n",
    "            if (new_w_end-new_w_start)<self.output_size:\n",
    "                new_w_end += (self.output_size-(new_w_end-new_w_start))\n",
    "            elif (new_w_end-new_w_start)>self.output_size:\n",
    "                new_w_end -= ((new_w_end-new_w_start)-self.output_size)\n",
    "            img = img[:, new_w_start:new_w_end]\n",
    "        elif new_w<self.output_size:\n",
    "            mid = new_w//2\n",
    "            new_w_start = self.output_size//2-mid\n",
    "            new_w_end = new_w_start+new_w\n",
    "            filled_img = np.zeros((self.output_size, self.output_size, img.shape[2]))\n",
    "            filled_img[:, new_w_start:new_w_end] = img[:, :]\n",
    "            img = filled_img\n",
    "        if self.debug:\n",
    "            root_dir = dataset.root_dir \n",
    "            copy_dir = root_dir+'rescaled/'\n",
    "            img_name = sample['name']\n",
    "            if not os.path.isdir(copy_dir):\n",
    "                os.mkdir(copy_dir)\n",
    "            label_dir_name = img_name.split('\\\\')[0]\n",
    "            label_dir_path = copy_dir + label_dir_name + '/'\n",
    "            if not os.path.isdir(label_dir_path):\n",
    "                os.mkdir(label_dir_path)\n",
    "            path_to_rescaled_img = copy_dir + img_name\n",
    "            io.imsave(path_to_rescaled_img, img, check_contrast=False)\n",
    "        \n",
    "        return {'image': img, 'label': label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'label': torch.from_numpy(label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ProjectDataset(\n",
    "    csv_file='D:/pravesh/Concordia/2021-Winter/COMP-6721-Intro_To_AI/project/comp-6721-project/label_info.csv'\n",
    "    ,root_dir='D:/pravesh/Concordia/2021-Winter/COMP-6721-Intro_To_AI/project/1/dataset/images/')\n",
    "print(len(dataset))\n",
    "\n",
    "output_height = 512\n",
    "\n",
    "scale = Rescale(output_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "counts = np.zeros((100, ))\n",
    "label_counts = {0: np.zeros((100, ))\n",
    "                , 1: np.zeros((100, ))\n",
    "                , 2:np.zeros((100, ))}\n",
    "\n",
    "dataset.debug=False\n",
    "scale.debug=True\n",
    "t1 = time.time()\n",
    "max_w, max_h = 0, 0\n",
    "min_w, min_h = 1000000, 100000\n",
    "count = 0\n",
    "for k in range(5):\n",
    "    sample = dataset[k]\n",
    "    sample_image = sample['image']\n",
    "    label = sample['label'][0]\n",
    "    print(sample['name'], sample_image.shape)\n",
    "    \n",
    "    h, w, _ = sample_image.shape\n",
    "    if w>max_w:\n",
    "        max_w = w\n",
    "    if h>max_h:\n",
    "        max_h = h\n",
    "    if w<min_w:\n",
    "        min_w = w\n",
    "    if h<min_h:\n",
    "        min_h = h\n",
    "    index = w//100\n",
    "    counts[index]+=1\n",
    "    label_counts[label][index]+=1\n",
    "    \n",
    "    transformed_sample = scale(sample)\n",
    "    image = transformed_sample['image']\n",
    "    print(sample['name'], image.shape)\n",
    "    plt.figure()\n",
    "#     image = image.astype(np.uint8)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    count+=1\n",
    "    if count%100==0:\n",
    "        gc.collect()\n",
    "        print(count)\n",
    "        print((time.time()-t1))\n",
    "    \n",
    "print((time.time()-t1))\n",
    "print(max_w, max_h, min_w, min_h)\n",
    "print(counts)\n",
    "for key in label_counts:\n",
    "    print(key, label_counts[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = ProjectDataset(csv_file='D:/pravesh/Concordia/2021-Winter/COMP-6721-Intro_To_AI/project/comp-6721-project/label_info.csv'\n",
    "                                     ,root_dir='D:/pravesh/Concordia/2021-Winter/COMP-6721-Intro_To_AI/project/1/dataset/images/'\n",
    "                                     ,transform=transforms.Compose([\n",
    "                                         Rescale(512)\n",
    "                                         ,ToTensor()\n",
    "                                     ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(dataset)\n",
    "X = np.zeros((dataset_size, output_height, output_height, 3), dtype=np.float32)\n",
    "y = np.zeros((dataset_size, 1), dtype=np.int32)\n",
    "for k in range(dataset_size):\n",
    "    sample = dataset[k]\n",
    "    sample_image, label = sample['image'], sample['label'][0]\n",
    "    \n",
    "    transformed_sample = scale(sample)\n",
    "    image = transformed_sample['image']\n",
    "    X[k] = image\n",
    "    y[k] = label\n",
    "np.savez_compressed('X.npz', X)\n",
    "np.savez_compressed('y.npz', y)\n",
    "print('Done converting to Numpy arrays')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:comp6721]",
   "language": "python",
   "name": "conda-env-comp6721-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
