{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>\n",
    "COMP-6721 | 2021-Winter\n",
    "Project | Part-1\n",
    "\n",
    "Pravesh Gupta | 40152506\n",
    "Vikramjeet Singh | \n",
    "Manjot Kaur Dherdi | \n",
    "</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Installing required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\installations\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: future in d:\\installations\\anaconda3\\lib\\site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in d:\\installations\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: dataclasses in d:\\installations\\anaconda3\\lib\\site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\lib\\site-packages (from torch) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in d:\\installations\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: torch==1.7.0 in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (1.7.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: future in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (3.7.4.2)\n",
      "Requirement already satisfied: dataclasses in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (0.6)\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\installations\\anaconda3\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\installations\\anaconda3\\lib\\site-packages (from pandas) (1.18.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\installations\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\installations\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\installations\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in d:\\installations\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: torch==1.7.0 in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (1.7.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\installations\\anaconda3\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: future in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (3.7.4.2)\n",
      "Requirement already satisfied: dataclasses in d:\\installations\\anaconda3\\lib\\site-packages (from torch==1.7.0->torchvision) (0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in d:\\installations\\anaconda3\\lib\\site-packages (0.16.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (1.1.1)\n",
      "Requirement already satisfied: pillow>=4.3.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (7.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (2.9.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (3.2.2)\n",
      "Requirement already satisfied: scipy>=0.19.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (1.5.0)\n",
      "Requirement already satisfied: networkx>=2.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-image) (2.4)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\installations\\anaconda3\\lib\\site-packages (from PyWavelets>=0.4.0->scikit-image) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\installations\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\installations\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in d:\\installations\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in d:\\installations\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in d:\\installations\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
      "Requirement already satisfied: six in d:\\installations\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\lib\\site-packages (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in d:\\installations\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in d:\\installations\\anaconda3\\lib\\site-packages (from sklearn) (0.23.1)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.18.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\installations\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\installations\\anaconda3\\lib\\site-packages (7.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Required Dataset Folder Heirarchy:\n",
    "<pre>\n",
    "{dataset_folder}\n",
    "--images\n",
    "----mask\n",
    "-------{mask images}\n",
    "----no_mask\n",
    "-------{no mask images}\n",
    "----not_person\n",
    "-------{not person images}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Defining Pytorch dataset Representation and data transaformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset\n",
    "import torch as torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io as sk_io, transform as sk_transform\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "    \n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size, debug=False, export_path=None):\n",
    "        assert isinstance(output_size, (int))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img_data):\n",
    "        img_arr = np.array(img_data)\n",
    "        h, w, c = img_arr.shape\n",
    "        isAlreadyScaled = (h==self.output_size and w==self.output_size)\n",
    "        \n",
    "        if not isAlreadyScaled:\n",
    "            scale_factor = float(self.output_size)/img_arr.shape[0]\n",
    "            img_arr = (sk_transform.rescale(img_arr, (scale_factor, scale_factor, 1))*255).astype(np.uint8)\n",
    "\n",
    "            new_w = img_arr.shape[1]\n",
    "\n",
    "            # Clipping or filling\n",
    "            if new_w>self.output_size:\n",
    "                mid = new_w//2\n",
    "                new_w_start = mid-self.output_size//2\n",
    "                new_w_end = mid+self.output_size//2\n",
    "\n",
    "                if (new_w_end-new_w_start)<self.output_size:\n",
    "                    new_w_end += (self.output_size-(new_w_end-new_w_start))\n",
    "                elif (new_w_end-new_w_start)>self.output_size:\n",
    "                    new_w_end -= ((new_w_end-new_w_start)-self.output_size)\n",
    "                img_arr = img_arr[:, new_w_start:new_w_end]\n",
    "            elif new_w<self.output_size:\n",
    "                mid = new_w//2\n",
    "                new_w_start = self.output_size//2-mid\n",
    "                new_w_end = new_w_start+new_w\n",
    "                filled_img_arr = np.zeros((self.output_size, self.output_size, img_arr.shape[2]), dtype=np.uint8)\n",
    "                filled_img_arr[:, new_w_start:new_w_end] = img_arr[:, :]\n",
    "                img_arr = filled_img_arr\n",
    "        return Image.fromarray(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Compose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b4156aa933c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m data_transform = Compose([\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mRescale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrescaled_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Compose' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(6721)\n",
    "\n",
    "rescaled_size=512\n",
    "\n",
    "scaler = Rescale(rescaled_size)\n",
    "\n",
    "data_dir = 'D:/Courses/COMP 6721 Applied AI/Project/Iteration1/final/images_original/'\n",
    "copy_dir = 'D:/Courses/COMP 6721 Applied AI/Project/Iteration1/final/images_rescaled/'\n",
    "\n",
    "if not os.path.isdir(copy_dir):\n",
    "    os.mkdir(copy_dir)\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "print(f'Dataset: size: {len(dataset)}, class labels: {dataset.class_to_idx}')\n",
    "\n",
    "label_to_class_dir = {}\n",
    "for class_name in dataset.class_to_idx:\n",
    "    label = dataset.class_to_idx[class_name]\n",
    "    class_dir = copy_dir + class_name + '/'\n",
    "    label_to_class_dir[label] = class_dir\n",
    "    if not os.path.isdir(class_dir):\n",
    "        os.mkdir(class_dir)\n",
    "\n",
    "for i in range(len(dataset.imgs)):\n",
    "    url, label = dataset.imgs[i]\n",
    "    img_name = url.split('\\\\')[1]\n",
    "    img_file_path = label_to_class_dir[label] + img_name\n",
    "    \n",
    "    if not os.path.isfile(img_file_path):\n",
    "        item = dataset.__getitem__(i)\n",
    "        rescaled_img = item[0]\n",
    "        rescaled_img.save(img_file_path, check_contrast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Creating dataset and data loaders for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_per_class = 600\n",
    "test_images_per_class = 100\n",
    "\n",
    "def get_train_test_indices(dataset_targets):\n",
    "    test_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        test_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    train_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        train_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    targets = np.array(dataset.targets, dtype=np.int32)\n",
    "    target_indices = np.where(targets!=None)[0]\n",
    "    np.random.shuffle(target_indices)\n",
    "\n",
    "    for i in target_indices:\n",
    "        label = dataset.targets[i]\n",
    "        if test_indices_map[label]['count']<test_images_per_class:\n",
    "            test_indices_map[label]['indices'] = np.append(test_indices_map[label]['indices'], i)\n",
    "            test_indices_map[label]['count']+=1\n",
    "        elif train_indices_map[label]['count']<train_images_per_class:\n",
    "            train_indices_map[label]['indices'] = np.append(train_indices_map[label]['indices'], i)\n",
    "            train_indices_map[label]['count']+=1\n",
    "\n",
    "    test_indices = np.array([], dtype=np.int32)\n",
    "    train_indices = np.array([], dtype=np.int32)\n",
    "\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        label_test_indices = test_indices_map[label]['indices']\n",
    "        test_indices = np.append(test_indices, label_test_indices)\n",
    "\n",
    "        label_train_indices = train_indices_map[label]['indices']\n",
    "        train_indices = np.append(train_indices, label_train_indices)\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: size: 2140, class labels: {'mask': 0, 'no_mask': 1, 'not_person': 2}\n",
      "Selected dataset: size: 2100\n",
      "Mean: tensor([0.4247, 0.3927, 0.3742])\n",
      "Std: tensor([0.2831, 0.2735, 0.2845])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(6721)\n",
    "\n",
    "rescaled_size=512\n",
    "data_dir = 'D:/Courses/COMP 6721 Applied AI/Project/images_rescaled_filtered_2100/'\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "    , ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "print(f'Dataset: size: {len(dataset)}, class labels: {dataset.class_to_idx}')\n",
    "\n",
    "train_indices, test_indices = get_train_test_indices(dataset.targets)\n",
    "selected_indices = np.concatenate((train_indices, test_indices))\n",
    "\n",
    "selected_dataset = Subset(dataset, selected_indices)\n",
    "print(f'Selected dataset: size: {len(selected_dataset)}')\n",
    "selected_dataloader = DataLoader(selected_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "means = torch.tensor([])\n",
    "stds = torch.tensor([])\n",
    "for i, (data, labels) in enumerate(selected_dataloader):\n",
    "    gc.collect()\n",
    "    batch_mean = torch.mean(data, axis=(0, 2, 3))\n",
    "    batch_std = torch.std(data, axis=(0, 2, 3))\n",
    "    means = torch.cat((means, batch_mean.unsqueeze(0)))\n",
    "    stds = torch.cat((stds, batch_std.unsqueeze(0)))\n",
    "mean = torch.mean(means, axis=0)\n",
    "std = torch.mean(stds, axis=0)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: size: 1800\n",
      "Train X | y batch shapes : (torch.Size([8, 3, 512, 512]), torch.Size([8]))\n",
      "Test dataset: size: 300\n",
      "Test X | y batch shapes : (torch.Size([8, 3, 512, 512]), torch.Size([8]))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "    , ToTensor()\n",
    "    , Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "print(f'Train dataset: size: {len(train_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "for data, labels in train_dataloader:\n",
    "    print(f'Train X | y batch shapes : {data.shape, labels.shape}')\n",
    "    break\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "print(f'Test dataset: size: {len(test_dataset)}')\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "for data, labels in test_dataloader:\n",
    "    print(f'Test X | y batch shapes : {data.shape, labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Module, Conv2d, MaxPool2d, Linear, ReLU, Dropout, BatchNorm2d, LeakyReLU, AdaptiveAvgPool2d, Flatten\n",
    "\n",
    "class ProjectModel(Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ProjectModel, self).__init__()\n",
    "        self.module = Sequential(\n",
    "            Conv2d(3, 32, kernel_size=3, stride=1, padding=2)\n",
    "            , BatchNorm2d(32)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(32)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            , Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(64)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(64)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            , AdaptiveAvgPool2d(output_size=(12, 12))\n",
    "            \n",
    "            , Flatten()\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=64*12*12, out_features=4096, bias=True)\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=4096, out_features=512, bias=True)\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.module(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Integrity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0051,  0.1050, -0.0900],\n",
      "        [ 0.0521,  0.1353,  0.2767],\n",
      "        [-0.0529,  0.0094, -0.0350],\n",
      "        [-0.0707, -0.0192,  0.1423],\n",
      "        [-0.2112,  0.2078,  0.1108],\n",
      "        [ 0.0579,  0.1619, -0.3096],\n",
      "        [ 0.1580, -0.0344, -0.2874],\n",
      "        [-0.0600,  0.0801, -0.1049]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 2, 1, 2, 1, 1, 0, 1])\n",
      "Accuracy: 62.5\n",
      "Model test passed.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = ProjectModel()\n",
    "\n",
    "# net = net.to(device)\n",
    "\n",
    "# Model Unit test\n",
    "total_labels = 0\n",
    "correct_labels = 0\n",
    "for data, labels in train_dataloader:\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    outputs = net(data)\n",
    "    print(outputs)\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "    print(y_pred)\n",
    "    total_labels+=labels.size(0)\n",
    "    correct_labels += (y_pred==labels).sum().item()\n",
    "    break\n",
    "\n",
    "print(f'Accuracy: {(correct_labels/total_labels)*100}')\n",
    "print('Model test passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProjectModel(\n",
       "  (module): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): AdaptiveAvgPool2d(output_size=(12, 12))\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Dropout(p=0.2, inplace=False)\n",
       "    (17): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (18): Dropout(p=0.2, inplace=False)\n",
       "    (19): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (20): Dropout(p=0.2, inplace=False)\n",
       "    (21): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_linear_layer_weights(layer):\n",
    "    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "def initialize_conv2d_layer_weights(layer):\n",
    "    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    \n",
    "def initialize_model_weights(module):\n",
    "    if isinstance(module, ProjectModel):\n",
    "        return\n",
    "    elif isinstance(module, Sequential):\n",
    "        return\n",
    "    if isinstance(module, Conv2d):\n",
    "        initialize_conv2d_layer_weights(module)\n",
    "    if isinstance(module, Linear):\n",
    "        initialize_linear_layer_weights(module)\n",
    "\n",
    "\n",
    "net = ProjectModel()\n",
    "net.apply(initialize_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "\n",
    "num_epoch = 2\n",
    "lr = 1e-3\n",
    "momentum = 0.5\n",
    "\n",
    "loss_evaluater = CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "After 0 batches: [time:0.0m 18.49634027481079s, Accuracy: 25.0]\n",
      "After 10 batches: [time:2.0m 5.9201366901397705s, Accuracy: 30.681818181818183]\n",
      "After 20 batches: [time:3.0m 49.31046223640442s, Accuracy: 37.5]\n",
      "After 30 batches: [time:5.0m 23.248998165130615s, Accuracy: 42.33870967741936]\n",
      "After 40 batches: [time:7.0m 2.6996042728424072s, Accuracy: 44.51219512195122]\n",
      "After 50 batches: [time:8.0m 42.663538694381714s, Accuracy: 46.07843137254902]\n",
      "After 60 batches: [time:10.0m 15.241950988769531s, Accuracy: 43.64754098360656]\n",
      "After 70 batches: [time:11.0m 47.54093933105469s, Accuracy: 43.485915492957744]\n",
      "After 80 batches: [time:13.0m 20.217180490493774s, Accuracy: 43.98148148148148]\n",
      "After 90 batches: [time:14.0m 54.583351850509644s, Accuracy: 45.32967032967033]\n",
      "After 100 batches: [time:16.0m 27.7591335773468s, Accuracy: 45.66831683168317]\n",
      "After 110 batches: [time:18.0m 51.67407178878784s, Accuracy: 47.072072072072075]\n",
      "After 120 batches: [time:20.0m 34.180922985076904s, Accuracy: 47.72727272727273]\n",
      "After 130 batches: [time:22.0m 5.7792370319366455s, Accuracy: 48.282442748091604]\n",
      "After 140 batches: [time:23.0m 49.030372858047485s, Accuracy: 49.11347517730496]\n",
      "After 150 batches: [time:25.0m 26.319190979003906s, Accuracy: 49.83443708609271]\n",
      "After 160 batches: [time:27.0m 15.316434621810913s, Accuracy: 50.85403726708074]\n",
      "After 170 batches: [time:29.0m 11.214702844619751s, Accuracy: 51.24269005847953]\n",
      "After 180 batches: [time:31.0m 0.6137795448303223s, Accuracy: 51.03591160220995]\n",
      "After 190 batches: [time:32.0m 49.51963567733765s, Accuracy: 51.63612565445026]\n",
      "After 200 batches: [time:34.0m 31.782158374786377s, Accuracy: 51.61691542288557]\n",
      "After 210 batches: [time:36.0m 11.559189319610596s, Accuracy: 51.18483412322274]\n",
      "After 220 batches: [time:37.0m 49.96165895462036s, Accuracy: 51.414027149321264]\n",
      "Epoch 1: [time:38.0m 29.505996465682983s, Accuracy: 51.388888888888886]\n",
      "\n",
      "Epoch: 2\n",
      "After 0 batches: [time:0.0m 9.846355676651001s, Accuracy: 75.0]\n",
      "After 10 batches: [time:1.0m 47.3664915561676s, Accuracy: 55.68181818181818]\n",
      "After 20 batches: [time:3.0m 35.15164661407471s, Accuracy: 61.904761904761905]\n",
      "After 30 batches: [time:5.0m 21.014325857162476s, Accuracy: 60.08064516129033]\n",
      "After 40 batches: [time:7.0m 12.749064445495605s, Accuracy: 57.62195121951219]\n",
      "After 50 batches: [time:8.0m 57.86652183532715s, Accuracy: 57.107843137254896]\n",
      "After 60 batches: [time:10.0m 45.5032844543457s, Accuracy: 57.99180327868852]\n",
      "After 70 batches: [time:12.0m 33.91473913192749s, Accuracy: 57.74647887323944]\n",
      "After 80 batches: [time:14.0m 18.652408599853516s, Accuracy: 56.79012345679012]\n",
      "After 90 batches: [time:16.0m 7.246027946472168s, Accuracy: 57.692307692307686]\n",
      "After 100 batches: [time:18.0m 6.923098087310791s, Accuracy: 57.67326732673267]\n",
      "After 110 batches: [time:20.0m 7.545654773712158s, Accuracy: 57.54504504504504]\n",
      "After 120 batches: [time:22.0m 0.0627894401550293s, Accuracy: 57.231404958677686]\n",
      "After 130 batches: [time:23.0m 38.15238308906555s, Accuracy: 57.63358778625955]\n",
      "After 140 batches: [time:25.0m 17.38890242576599s, Accuracy: 58.06737588652482]\n",
      "After 150 batches: [time:26.0m 48.84112572669983s, Accuracy: 58.27814569536424]\n",
      "After 160 batches: [time:28.0m 45.37012267112732s, Accuracy: 58.54037267080745]\n",
      "After 170 batches: [time:31.0m 21.00789451599121s, Accuracy: 58.040935672514614]\n",
      "After 180 batches: [time:33.0m 2.9883878231048584s, Accuracy: 58.149171270718234]\n",
      "After 190 batches: [time:34.0m 38.90836524963379s, Accuracy: 58.57329842931938]\n",
      "After 200 batches: [time:36.0m 17.485597610473633s, Accuracy: 59.14179104477611]\n",
      "After 210 batches: [time:37.0m 56.1485435962677s, Accuracy: 59.47867298578199]\n",
      "After 220 batches: [time:39.0m 46.571900844573975s, Accuracy: 59.72850678733032]\n",
      "Epoch 2: [time:40.0m 32.4022581577301s, Accuracy: 59.833333333333336]\n",
      "\n",
      "Training time: 79.0m 1.9172477722167969s\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "\n",
    "t = time.time()\n",
    "for i in range(num_epoch):\n",
    "    print(f'Epoch: {i+1}')\n",
    "    t_epoch = time.time()\n",
    "    total_labels = 0\n",
    "    correctly_pred = 0\n",
    "    t_batch = time.time()\n",
    "    for j, (data, labels) in enumerate(train_dataloader):\n",
    "#         torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "#         data, labels = data.to(device), labels.to(device)\n",
    "        X, y = data, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = net(X)\n",
    "        loss = loss_evaluater(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        total_labels += y.size(0)\n",
    "        correctly_pred += (y_pred==y).sum().item()\n",
    "        current_acc = (correctly_pred/total_labels)\n",
    "        if j%10==0:\n",
    "            seconds_passed = time.time()-t_batch\n",
    "            print(f'After {j} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "    epoch_acc = (correctly_pred/total_labels)\n",
    "    seconds_passed = time.time()-t_epoch\n",
    "    print(f'Epoch {i+1}: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {epoch_acc*100}]')\n",
    "    print()\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Training time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 3.3356618881225586s, Accuracy: 100.0]\n",
      "After 5 batches: [time:0.0m 18.053566694259644s, Accuracy: 83.33333333333334]\n",
      "After 10 batches: [time:0.0m 33.65128540992737s, Accuracy: 82.95454545454545]\n",
      "After 15 batches: [time:0.0m 50.005255460739136s, Accuracy: 83.59375]\n",
      "After 20 batches: [time:1.0m 7.13040828704834s, Accuracy: 82.73809523809523]\n",
      "After 25 batches: [time:1.0m 23.544406414031982s, Accuracy: 83.17307692307693]\n",
      "After 30 batches: [time:1.0m 40.080196142196655s, Accuracy: 81.45161290322581]\n",
      "After 35 batches: [time:1.0m 57.712846755981445s, Accuracy: 81.94444444444444]\n",
      "After 40 batches: [time:2.0m 14.771364212036133s, Accuracy: 82.6219512195122]\n",
      "After 45 batches: [time:2.0m 31.99657654762268s, Accuracy: 82.33695652173914]\n",
      "After 50 batches: [time:2.0m 49.20467662811279s, Accuracy: 81.61764705882352]\n",
      "After 55 batches: [time:3.0m 6.0036303997039795s, Accuracy: 80.35714285714286]\n",
      "After 60 batches: [time:3.0m 21.27405309677124s, Accuracy: 78.48360655737704]\n",
      "After 65 batches: [time:3.0m 37.87223172187805s, Accuracy: 78.21969696969697]\n",
      "After 70 batches: [time:3.0m 54.969146966934204s, Accuracy: 78.87323943661971]\n",
      "After 75 batches: [time:4.0m 11.498555660247803s, Accuracy: 79.27631578947368]\n",
      "After 80 batches: [time:4.0m 28.237923860549927s, Accuracy: 78.85802469135803]\n",
      "After 85 batches: [time:4.0m 45.52359104156494s, Accuracy: 79.21511627906976]\n",
      "After 90 batches: [time:5.0m 2.9486162662506104s, Accuracy: 79.67032967032966]\n",
      "After 95 batches: [time:5.0m 19.882969617843628s, Accuracy: 79.55729166666666]\n",
      "After 100 batches: [time:5.0m 36.39998769760132s, Accuracy: 79.33168316831683]\n",
      "After 105 batches: [time:5.0m 53.0203857421875s, Accuracy: 79.12735849056604]\n",
      "After 110 batches: [time:6.0m 9.506227493286133s, Accuracy: 79.27927927927928]\n",
      "After 115 batches: [time:6.0m 26.533411741256714s, Accuracy: 79.3103448275862]\n",
      "After 120 batches: [time:6.0m 43.77591037750244s, Accuracy: 79.23553719008265]\n",
      "After 125 batches: [time:7.0m 0.5691916942596436s, Accuracy: 79.26587301587301]\n",
      "After 130 batches: [time:7.0m 17.33233666419983s, Accuracy: 79.1030534351145]\n",
      "After 135 batches: [time:7.0m 34.070141077041626s, Accuracy: 79.22794117647058]\n",
      "After 140 batches: [time:7.0m 50.75395488739014s, Accuracy: 79.07801418439716]\n",
      "After 145 batches: [time:8.0m 7.953128099441528s, Accuracy: 79.45205479452055]\n",
      "After 150 batches: [time:8.0m 25.418269634246826s, Accuracy: 79.63576158940397]\n",
      "After 155 batches: [time:8.0m 42.968403339385986s, Accuracy: 79.7275641025641]\n",
      "After 160 batches: [time:8.0m 59.920493364334106s, Accuracy: 79.8136645962733]\n",
      "After 165 batches: [time:9.0m 17.03219723701477s, Accuracy: 79.81927710843374]\n",
      "After 170 batches: [time:9.0m 33.957173347473145s, Accuracy: 79.60526315789474]\n",
      "After 175 batches: [time:9.0m 51.337926149368286s, Accuracy: 79.82954545454545]\n",
      "After 180 batches: [time:10.0m 8.947852373123169s, Accuracy: 79.97237569060773]\n",
      "After 185 batches: [time:10.0m 26.115593194961548s, Accuracy: 79.83870967741935]\n",
      "After 190 batches: [time:10.0m 43.02615165710449s, Accuracy: 79.97382198952879]\n",
      "Training accuracy: 80.06451612903226\n",
      "Trainig evaluation time: 10.0m 52.250762701034546s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "t = time.time()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_labels = 0\n",
    "    correctly_pred = 0\n",
    "    t_batch = time.time()\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "#         data,labels = data.to(device), labels.to(device)\n",
    "        gc.collect()\n",
    "        X, y = data, labels\n",
    "        output = net(X)\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        total_labels += y.size(0)\n",
    "        correctly_pred += (y_pred==y).sum().item()\n",
    "        current_acc = (correctly_pred/total_labels)\n",
    "        if i%5==0:\n",
    "            seconds_passed = time.time()-t_batch\n",
    "            print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "    train_acc = (correctly_pred/total_labels)\n",
    "    print(f'Training accuracy: {train_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "\n",
    "print(f'Trainig evaluation time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 3.2525296211242676s, Accuracy: 100.0]\n",
      "After 5 batches: [time:6.0m 56.673192262649536s, Accuracy: 77.08333333333334]\n",
      "After 10 batches: [time:10.0m 43.78805637359619s, Accuracy: 77.27272727272727]\n",
      "After 15 batches: [time:11.0m 8.210622310638428s, Accuracy: 77.34375]\n",
      "After 20 batches: [time:11.0m 29.31381869316101s, Accuracy: 74.40476190476191]\n",
      "After 25 batches: [time:11.0m 46.50768709182739s, Accuracy: 73.5576923076923]\n",
      "After 30 batches: [time:11.0m 59.85336470603943s, Accuracy: 74.59677419354838]\n",
      "After 35 batches: [time:12.0m 12.875742435455322s, Accuracy: 74.30555555555556]\n",
      "Test accuracy: 73.33333333333333\n",
      "Test time: 12.0m 17.54648995399475s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "t = time.time()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "total_labels = 0\n",
    "correctly_pred = 0\n",
    "t_batch = time.time()\n",
    "test_batch=[]\n",
    "pred_batch=[]\n",
    "for i, (data, labels) in enumerate(test_dataloader):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    gc.collect()\n",
    "    X, y = data, labels\n",
    "    test_batch.extend(y.tolist())\n",
    "    output = net(X)\n",
    "    y_pred = torch.argmax(output, dim=1)\n",
    "    pred_batch.extend(y_pred.tolist())\n",
    "    total_labels += y.size(0)\n",
    "    correctly_pred += (y_pred==y).sum().item()\n",
    "    current_acc = (correctly_pred/total_labels)\n",
    "    if i%5==0:\n",
    "        seconds_passed = time.time()-t_batch\n",
    "        print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "\n",
    "test_acc = (correctly_pred/total_labels)\n",
    "print(f'Test accuracy: {test_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Test time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Saving pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "file_path = 'D:/Courses/COMP 6721 Applied AI/Project/net_6.pt'\n",
    "\n",
    "state = {\n",
    "    'epoch': num_epoch,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr': lr,\n",
    "    'momentum': momentum\n",
    "}\n",
    "\n",
    "torch.save(state, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "state = torch.load('D:/Courses/COMP 6721 Applied AI/Project/net_6.pt')\n",
    "\n",
    "loaded_net = ProjectModel()\n",
    "loaded_loss_evaluater = CrossEntropyLoss()\n",
    "loaded_optimizer = SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "loaded_num_epoch = state['epoch']\n",
    "loaded_lr = state['lr']\n",
    "loaded_momentum = state['momentum']\n",
    "loaded_net.load_state_dict(state['state_dict'])\n",
    "loaded_optimizer.load_state_dict(state['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing loaded pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 13.903450965881348s, Accuracy: 87.5]\n",
      "After 5 batches: [time:1.0m 13.37346076965332s, Accuracy: 79.16666666666666]\n",
      "After 10 batches: [time:1.0m 33.71850538253784s, Accuracy: 75.0]\n",
      "After 15 batches: [time:1.0m 54.34468412399292s, Accuracy: 73.4375]\n",
      "After 20 batches: [time:2.0m 12.677175998687744s, Accuracy: 73.21428571428571]\n",
      "After 25 batches: [time:2.0m 31.080754280090332s, Accuracy: 73.07692307692307]\n",
      "After 30 batches: [time:2.0m 49.37603998184204s, Accuracy: 72.17741935483872]\n",
      "After 35 batches: [time:3.0m 9.234417200088501s, Accuracy: 71.875]\n",
      "Test accuracy: 71.33333333333334\n",
      "Test time: 3.0m 14.898915529251099s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "\n",
    "loaded_net.eval()\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "total_labels = 0\n",
    "correctly_pred = 0\n",
    "t_batch = time.time()\n",
    "test_batch=[]\n",
    "pred_batch=[]\n",
    "precision_sum = 0.0\n",
    "recall_sum = 0.0\n",
    "f1_sum = 0.0\n",
    "\n",
    "for i, (data, labels) in enumerate(test_dataloader):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    gc.collect()\n",
    "    X, y = data, labels\n",
    "    test_batch.extend(y.tolist())\n",
    "    output = loaded_net(X)\n",
    "    y_pred = torch.argmax(output, dim=1)\n",
    "    pred_batch.extend(y_pred.tolist())\n",
    "    total_labels += y.size(0)\n",
    "    correctly_pred += (y_pred==y).sum().item()\n",
    "    current_acc = (correctly_pred/total_labels)\n",
    "    if i%5==0:\n",
    "        seconds_passed = time.time()-t_batch\n",
    "        print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "\n",
    "test_acc = (correctly_pred/total_labels)\n",
    "print(f'Test accuracy: {test_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Test time: {seconds_passed//60}m {seconds_passed%60}s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index=dataset.class_to_idx['mask']\n",
    "no_mask_index=dataset.class_to_idx['no_mask']\n",
    "not_person_index=dataset.class_to_idx['not_person']\n",
    "\n",
    "confusion_metrics_final=metrics.confusion_matrix(test_batch,pred_batch)\n",
    "precision_final=metrics.precision_score(test_batch, pred_batch, average=None)\n",
    "recall_final=metrics.recall_score(test_batch, pred_batch, average=None)\n",
    "f1_final=metrics.f1_score(test_batch, pred_batch, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the model is 71.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for the model is {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the class mask is 75.86206896551724 %\n",
      "Precision for the class no_mask is 56.41025641025641 %\n",
      "Precision for the class not_person is 95.34883720930233 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision for the class mask is {precision_final[mask_index]*100} %')\n",
    "print(f'Precision for the class no_mask is {precision_final[no_mask_index]*100} %')\n",
    "print(f'Precision for the class not_person is {precision_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for the class mask is 44.0 %\n",
      "Recall for the class no_mask is 88.0 %\n",
      "Recall for the class not_person is 82.0 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Recall for the class mask is {recall_final[mask_index]*100} %')\n",
    "print(f'Recall for the class no_mask is {recall_final[no_mask_index]*100} %')\n",
    "print(f'Recall for the class not_person is {recall_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the class mask is 55.69620253164557 %\n",
      "F1 Score for the class no_mask is 68.75 %\n",
      "F1 Score for the class not_person is 88.17204301075269 %\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for the class mask is {f1_final[mask_index]*100} %')\n",
    "print(f'F1 Score for the class no_mask is {f1_final[no_mask_index]*100} %')\n",
    "print(f'F1 Score for the class not_person is {f1_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1bb2b692100>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(test_batch, pred_batch, normalize='all')*100\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['mask', 'no_mask', 'not_person'])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Confusion Matrix(%)')\n",
    "cmd.plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
