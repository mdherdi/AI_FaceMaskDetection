{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "<b>\n",
    "COMP-6721 | 2021-Winter\n",
    "Project | Part-1\n",
    "\n",
    "Pravesh Gupta | 40152506\n",
    "Vikramjeet Singh | 40134477\n",
    "Manjot Kaur Dherdi | 40107905\n",
    "</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Installing required python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch) (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.9.1-cp39-cp39-win_amd64.whl (852 kB)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.20.2)\n",
      "Requirement already satisfied: torch==1.8.1 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Collecting pillow>=4.1.1\n",
      "  Downloading Pillow-8.2.0-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "Requirement already satisfied: typing-extensions in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-8.2.0 torchvision-0.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.2.4-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from pandas) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.2.4 pytz-2021.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: torch==1.8.1 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (1.20.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.18.1-cp39-cp39-win_amd64.whl (12.2 MB)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from scikit-image) (8.2.0)\n",
      "Collecting scipy>=1.0.1\n",
      "  Downloading scipy-1.6.2-cp39-cp39-win_amd64.whl (32.7 MB)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "Collecting networkx>=2.0\n",
      "  Using cached networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting imageio>=2.3.0\n",
      "  Using cached imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "Collecting matplotlib!=3.0.0,>=2.0.0\n",
      "  Downloading matplotlib-3.4.1-cp39-cp39-win_amd64.whl (7.1 MB)\n",
      "Requirement already satisfied: numpy>=1.16.5 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from scikit-image) (1.20.2)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2021.4.8-py3-none-any.whl (165 kB)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp39-cp39-win_amd64.whl (51 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "Collecting decorator<5,>=4.3\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, decorator, cycler, tifffile, scipy, PyWavelets, networkx, matplotlib, imageio, scikit-image\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.0.6\n",
      "    Uninstalling decorator-5.0.6:\n",
      "      Successfully uninstalled decorator-5.0.6\n",
      "Successfully installed PyWavelets-1.1.1 cycler-0.10.0 decorator-4.4.2 imageio-2.9.0 kiwisolver-1.3.1 matplotlib-3.4.1 networkx-2.5.1 pyparsing-2.4.7 scikit-image-0.18.1 scipy-1.6.2 tifffile-2021.4.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (1.20.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp39-cp39-win_amd64.whl (6.9 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from scikit-learn->sklearn) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Collecting joblib>=0.11\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=bf0934a3f7dbcc9738c51cc3154cb2c13be8934cf9161489ab7765da733fd710\n",
      "Note: you may need to restart the kernel to use updated packages.  Stored in directory: c:\\users\\prave\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.0.1 scikit-learn-0.24.1 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\installations\\anaconda3\\envs\\comp6721\\lib\\site-packages (8.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Required Dataset Folder Heirarchy:\n",
    "<pre>\n",
    "{dataset_folder}\n",
    "--images\n",
    "----mask\n",
    "-------{mask images}\n",
    "----no_mask\n",
    "-------{no mask images}\n",
    "----not_person\n",
    "-------{not person images}\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Defining Pytorch dataset Representation and data transaformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import VisionDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io as sk_io, transform as sk_transform\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "    \n",
    "class Rescale(object):\n",
    "    def __init__(self, output_size, debug=False, export_path=None):\n",
    "        assert isinstance(output_size, (int))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, img_data):\n",
    "        img_arr = np.array(img_data)\n",
    "        h, w, c = img_arr.shape\n",
    "        isAlreadyScaled = (h==self.output_size and w==self.output_size)\n",
    "        \n",
    "        if not isAlreadyScaled:\n",
    "            scale_factor = float(self.output_size)/img_arr.shape[0]\n",
    "            img_arr = (sk_transform.rescale(img_arr, (scale_factor, scale_factor, 1))*255).astype(np.uint8)\n",
    "\n",
    "            new_w = img_arr.shape[1]\n",
    "\n",
    "            # Clipping or filling\n",
    "            if new_w>self.output_size:\n",
    "                mid = new_w//2\n",
    "                new_w_start = mid-self.output_size//2\n",
    "                new_w_end = mid+self.output_size//2\n",
    "\n",
    "                if (new_w_end-new_w_start)<self.output_size:\n",
    "                    new_w_end += (self.output_size-(new_w_end-new_w_start))\n",
    "                elif (new_w_end-new_w_start)>self.output_size:\n",
    "                    new_w_end -= ((new_w_end-new_w_start)-self.output_size)\n",
    "                img_arr = img_arr[:, new_w_start:new_w_end]\n",
    "            elif new_w<self.output_size:\n",
    "                mid = new_w//2\n",
    "                new_w_start = self.output_size//2-mid\n",
    "                new_w_end = new_w_start+new_w\n",
    "                filled_img_arr = np.zeros((self.output_size, self.output_size, img_arr.shape[2]), dtype=np.uint8)\n",
    "                filled_img_arr[:, new_w_start:new_w_end] = img_arr[:, :]\n",
    "                img_arr = filled_img_arr\n",
    "        return Image.fromarray(img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. (Optional) Prior conversion of image to speed up training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "rescaled_size=512\n",
    "\n",
    "scaler = Rescale(rescaled_size)\n",
    "\n",
    "data_dir = './data/images_original/'\n",
    "copy_dir = './data/images_rescaled/'\n",
    "\n",
    "if not os.path.isdir(copy_dir):\n",
    "    os.mkdir(copy_dir)\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "print(f'Dataset: size: {len(dataset)}, class labels: {dataset.class_to_idx}')\n",
    "\n",
    "label_to_class_dir = {}\n",
    "for class_name in dataset.class_to_idx:\n",
    "    label = dataset.class_to_idx[class_name]\n",
    "    class_dir = copy_dir + class_name + '/'\n",
    "    label_to_class_dir[label] = class_dir\n",
    "    if not os.path.isdir(class_dir):\n",
    "        os.mkdir(class_dir)\n",
    "\n",
    "for i in range(len(dataset.imgs)):\n",
    "    url, label = dataset.imgs[i]\n",
    "    img_name = url.split('\\\\')[1]\n",
    "    img_file_path = label_to_class_dir[label] + img_name\n",
    "    \n",
    "    if not os.path.isfile(img_file_path):\n",
    "        item = dataset.__getitem__(i)\n",
    "        rescaled_img = item[0]\n",
    "        rescaled_img.save(img_file_path, check_contrast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Creating dataset and data loaders for train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From whole dataset, split and find train/test dataset data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_per_class = 600\n",
    "test_images_per_class = 100\n",
    "\n",
    "def get_train_test_indices(dataset_targets):\n",
    "    test_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        test_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    train_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        train_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    targets = np.array(dataset.targets, dtype=np.int32)\n",
    "    target_indices = np.where(targets!=None)[0]\n",
    "    np.random.shuffle(target_indices)\n",
    "\n",
    "    for i in target_indices:\n",
    "        label = dataset.targets[i]\n",
    "        if test_indices_map[label]['count']<test_images_per_class:\n",
    "            test_indices_map[label]['indices'] = np.append(test_indices_map[label]['indices'], i)\n",
    "            test_indices_map[label]['count']+=1\n",
    "        elif train_indices_map[label]['count']<train_images_per_class:\n",
    "            train_indices_map[label]['indices'] = np.append(train_indices_map[label]['indices'], i)\n",
    "            train_indices_map[label]['count']+=1\n",
    "\n",
    "    test_indices = np.array([], dtype=np.int32)\n",
    "    train_indices = np.array([], dtype=np.int32)\n",
    "\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        label_test_indices = test_indices_map[label]['indices']\n",
    "        test_indices = np.append(test_indices, label_test_indices)\n",
    "\n",
    "        label_train_indices = train_indices_map[label]['indices']\n",
    "        train_indices = np.append(train_indices, label_train_indices)\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_per_class = 50\n",
    "\n",
    "def get_train_test_indices(dataset_targets):\n",
    "    test_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        test_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    train_indices_map = {}\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        train_indices_map[label] = {'indices': np.array([], dtype=np.int32), 'count': 0}\n",
    "\n",
    "    targets = np.array(dataset.targets, dtype=np.int32)\n",
    "    target_indices = np.where(targets!=None)[0]\n",
    "    np.random.shuffle(target_indices)\n",
    "\n",
    "    for i in target_indices:\n",
    "        label = dataset.targets[i]\n",
    "        if test_indices_map[label]['count']<test_images_per_class:\n",
    "            test_indices_map[label]['indices'] = np.append(test_indices_map[label]['indices'], i)\n",
    "            test_indices_map[label]['count']+=1\n",
    "        else:\n",
    "            train_indices_map[label]['indices'] = np.append(train_indices_map[label]['indices'], i)\n",
    "            train_indices_map[label]['count']+=1\n",
    "\n",
    "    test_indices = np.array([], dtype=np.int32)\n",
    "    train_indices = np.array([], dtype=np.int32)\n",
    "\n",
    "    for class_name in dataset.class_to_idx:\n",
    "        label = dataset.class_to_idx[class_name]\n",
    "        label_test_indices = test_indices_map[label]['indices']\n",
    "        test_indices = np.append(test_indices, label_test_indices)\n",
    "\n",
    "        label_train_indices = train_indices_map[label]['indices']\n",
    "        train_indices = np.append(train_indices, label_train_indices)\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating mean and std for Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: size: 1070, class labels: {'mask': 0, 'no_mask': 1, 'not_person': 2}\n",
      "Selected dataset: size: 1070\n",
      "Mean: tensor([0.4284, 0.3941, 0.3736])\n",
      "Std: tensor([0.2837, 0.2731, 0.2844])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import gc\n",
    "\n",
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "rescaled_size=512\n",
    "data_dir = './data/images_female/'\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "    , ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "print(f'Dataset: size: {len(dataset)}, class labels: {dataset.class_to_idx}')\n",
    "\n",
    "train_indices, test_indices = get_train_test_indices(dataset.targets)\n",
    "selected_indices = np.concatenate((train_indices, test_indices))\n",
    "\n",
    "selected_dataset = Subset(dataset, selected_indices)\n",
    "print(f'Selected dataset: size: {len(selected_dataset)}')\n",
    "selected_dataloader = DataLoader(selected_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "means = torch.tensor([])\n",
    "stds = torch.tensor([])\n",
    "for i, (data, labels) in enumerate(selected_dataloader):\n",
    "    gc.collect()\n",
    "    batch_mean = torch.mean(data, axis=(0, 2, 3))\n",
    "    batch_std = torch.std(data, axis=(0, 2, 3))\n",
    "    means = torch.cat((means, batch_mean.unsqueeze(0)))\n",
    "    stds = torch.cat((stds, batch_std.unsqueeze(0)))\n",
    "mean = torch.mean(means, axis=0)\n",
    "std = torch.mean(stds, axis=0)\n",
    "print(f'Mean: {mean}')\n",
    "print(f'Std: {std}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train and test dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: size: 920\n",
      "Train X | y batch shapes : (torch.Size([8, 3, 512, 512]), torch.Size([8]))\n",
      "Test dataset: size: 150\n",
      "Test X | y batch shapes : (torch.Size([8, 3, 512, 512]), torch.Size([8]))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "data_transform = Compose([\n",
    "    Rescale(rescaled_size)\n",
    "    , ToTensor()\n",
    "    , Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(\n",
    "    root=data_dir\n",
    "    ,transform=data_transform\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "print(f'Train dataset: size: {len(train_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "for data, labels in train_dataloader:\n",
    "    print(f'Train X | y batch shapes : {data.shape, labels.shape}')\n",
    "    break\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "print(f'Test dataset: size: {len(test_dataset)}')\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "for data, labels in test_dataloader:\n",
    "    print(f'Test X | y batch shapes : {data.shape, labels.shape}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Module, Conv2d, MaxPool2d, Linear, ReLU, Dropout, BatchNorm2d, LeakyReLU, AdaptiveAvgPool2d, Flatten\n",
    "\n",
    "class ProjectModel(Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ProjectModel, self).__init__()\n",
    "        self.module = Sequential(\n",
    "            Conv2d(3, 32, kernel_size=3, stride=1, padding=2)\n",
    "            , BatchNorm2d(32)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(32)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            , Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(64)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
    "            , BatchNorm2d(64)\n",
    "            , LeakyReLU(inplace=True)\n",
    "            \n",
    "            , MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            , AdaptiveAvgPool2d(output_size=(12, 12))\n",
    "            \n",
    "            , Flatten()\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=64*12*12, out_features=4096, bias=True)\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=4096, out_features=512, bias=True)\n",
    "            , Dropout(p=0.2, inplace=False)\n",
    "            , Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.module(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Integrity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1097,  0.4323, -0.0100],\n",
      "        [-0.0557,  0.2633,  0.0921],\n",
      "        [ 0.0248,  0.0986, -0.2997],\n",
      "        [-0.1664, -0.0431,  0.0599],\n",
      "        [ 0.1223, -0.2518,  0.0894],\n",
      "        [-0.1940, -0.0964, -0.4280],\n",
      "        [-0.3368,  0.0925, -0.0640],\n",
      "        [-0.0031,  0.2569, -0.0128]], grad_fn=<AddmmBackward>)\n",
      "tensor([1, 1, 1, 2, 0, 1, 1, 1])\n",
      "Accuracy: 25.0\n",
      "Model test passed.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net = ProjectModel()\n",
    "\n",
    "# net = net.to(device)\n",
    "\n",
    "# Model Unit test\n",
    "total_labels = 0\n",
    "correct_labels = 0\n",
    "for data, labels in train_dataloader:\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    outputs = net(data)\n",
    "    print(outputs)\n",
    "    y_pred = torch.argmax(outputs, dim=1)\n",
    "    print(y_pred)\n",
    "    total_labels+=labels.size(0)\n",
    "    correct_labels += (y_pred==labels).sum().item()\n",
    "    break\n",
    "\n",
    "print(f'Accuracy: {(correct_labels/total_labels)*100}')\n",
    "print('Model test passed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Model Parameter Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProjectModel(\n",
       "  (module): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): AdaptiveAvgPool2d(output_size=(12, 12))\n",
       "    (15): Flatten(start_dim=1, end_dim=-1)\n",
       "    (16): Dropout(p=0.2, inplace=False)\n",
       "    (17): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (18): Dropout(p=0.2, inplace=False)\n",
       "    (19): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (20): Dropout(p=0.2, inplace=False)\n",
       "    (21): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "def initialize_linear_layer_weights(layer):\n",
    "    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    torch.nn.init.zeros_(layer.bias)\n",
    "        \n",
    "def initialize_conv2d_layer_weights(layer):\n",
    "    torch.nn.init.xavier_uniform_(layer.weight)\n",
    "    \n",
    "def initialize_model_weights(module):\n",
    "    if isinstance(module, ProjectModel):\n",
    "        return\n",
    "    elif isinstance(module, Sequential):\n",
    "        return\n",
    "    if isinstance(module, Conv2d):\n",
    "        initialize_conv2d_layer_weights(module)\n",
    "    if isinstance(module, Linear):\n",
    "        initialize_linear_layer_weights(module)\n",
    "\n",
    "\n",
    "net = ProjectModel()\n",
    "net.apply(initialize_model_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameter Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "\n",
    "num_epoch = 10\n",
    "lr = 1e-3\n",
    "momentum = 0.5\n",
    "\n",
    "loss_evaluater = CrossEntropyLoss()\n",
    "optimizer = Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "After 0 batches: [time:0.0m 23.437934637069702s, Accuracy: 37.5]\n",
      "After 5 batches: [time:0.0m 58.35715985298157s, Accuracy: 45.83333333333333]\n",
      "After 10 batches: [time:1.0m 26.73293161392212s, Accuracy: 35.22727272727273]\n",
      "After 15 batches: [time:1.0m 56.97692632675171s, Accuracy: 37.5]\n",
      "After 20 batches: [time:2.0m 29.191585779190063s, Accuracy: 32.73809523809524]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "net.train()\n",
    "\n",
    "t = time.time()\n",
    "for i in range(num_epoch):\n",
    "    print(f'Epoch: {i+1}')\n",
    "    t_epoch = time.time()\n",
    "    total_labels = 0\n",
    "    correctly_pred = 0\n",
    "    t_batch = time.time()\n",
    "    for j, (data, labels) in enumerate(train_dataloader):\n",
    "#         torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "#         data, labels = data.to(device), labels.to(device)\n",
    "        X, y = data, labels\n",
    "        optimizer.zero_grad()\n",
    "        output = net(X)\n",
    "        loss = loss_evaluater(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        total_labels += y.size(0)\n",
    "        correctly_pred += (y_pred==y).sum().item()\n",
    "        current_acc = (correctly_pred/total_labels)\n",
    "        if j%5==0:\n",
    "            seconds_passed = time.time()-t_batch\n",
    "            print(f'After {j} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "    epoch_acc = (correctly_pred/total_labels)\n",
    "    seconds_passed = time.time()-t_epoch\n",
    "    print(f'Epoch {i+1}: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {epoch_acc*100}]')\n",
    "    print()\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Training time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 2.2898812294006348s, Accuracy: 62.5]\n",
      "After 5 batches: [time:0.0m 13.28598427772522s, Accuracy: 83.33333333333334]\n",
      "After 10 batches: [time:0.0m 23.75367259979248s, Accuracy: 77.27272727272727]\n",
      "After 15 batches: [time:0.0m 34.322410345077515s, Accuracy: 79.6875]\n",
      "After 20 batches: [time:0.0m 44.755594968795776s, Accuracy: 79.76190476190477]\n",
      "After 25 batches: [time:0.0m 55.32632064819336s, Accuracy: 82.6923076923077]\n",
      "After 30 batches: [time:1.0m 5.841709136962891s, Accuracy: 83.06451612903226]\n",
      "After 35 batches: [time:1.0m 16.541173934936523s, Accuracy: 81.94444444444444]\n",
      "After 40 batches: [time:1.0m 27.2585289478302s, Accuracy: 82.92682926829268]\n",
      "After 45 batches: [time:1.0m 37.807833194732666s, Accuracy: 81.52173913043478]\n",
      "After 50 batches: [time:1.0m 48.442392110824585s, Accuracy: 80.88235294117648]\n",
      "After 55 batches: [time:1.0m 58.936373710632324s, Accuracy: 81.91964285714286]\n",
      "After 60 batches: [time:2.0m 9.502629280090332s, Accuracy: 82.17213114754098]\n",
      "After 65 batches: [time:2.0m 20.067475080490112s, Accuracy: 82.38636363636364]\n",
      "After 70 batches: [time:2.0m 30.657769680023193s, Accuracy: 82.3943661971831]\n",
      "After 75 batches: [time:2.0m 42.32462239265442s, Accuracy: 82.56578947368422]\n",
      "After 80 batches: [time:2.0m 55.80312252044678s, Accuracy: 82.4074074074074]\n",
      "After 85 batches: [time:3.0m 8.522879123687744s, Accuracy: 81.54069767441861]\n",
      "After 90 batches: [time:3.0m 20.431737899780273s, Accuracy: 81.31868131868131]\n",
      "After 95 batches: [time:3.0m 32.98026895523071s, Accuracy: 81.25]\n",
      "After 100 batches: [time:3.0m 45.19900369644165s, Accuracy: 81.1881188118812]\n",
      "After 105 batches: [time:3.0m 58.55503606796265s, Accuracy: 81.60377358490565]\n",
      "After 110 batches: [time:4.0m 10.593944549560547s, Accuracy: 81.98198198198197]\n",
      "After 115 batches: [time:4.0m 23.2232346534729s, Accuracy: 81.35775862068965]\n",
      "After 120 batches: [time:4.0m 35.58244824409485s, Accuracy: 81.19834710743802]\n",
      "After 125 batches: [time:4.0m 47.62057399749756s, Accuracy: 81.54761904761905]\n",
      "After 130 batches: [time:5.0m 0.020206928253173828s, Accuracy: 81.58396946564885]\n",
      "After 135 batches: [time:5.0m 11.674289464950562s, Accuracy: 81.25]\n",
      "After 140 batches: [time:5.0m 23.479106664657593s, Accuracy: 81.38297872340425]\n",
      "After 145 batches: [time:5.0m 34.86311483383179s, Accuracy: 81.5068493150685]\n",
      "After 150 batches: [time:5.0m 46.87763023376465s, Accuracy: 81.45695364238411]\n",
      "After 155 batches: [time:5.0m 59.66323184967041s, Accuracy: 81.65064102564102]\n",
      "After 160 batches: [time:6.0m 11.190478801727295s, Accuracy: 81.67701863354037]\n",
      "After 165 batches: [time:6.0m 23.049373865127563s, Accuracy: 81.62650602409639]\n",
      "After 170 batches: [time:6.0m 35.83574414253235s, Accuracy: 81.65204678362574]\n",
      "After 175 batches: [time:6.0m 49.41688537597656s, Accuracy: 81.81818181818183]\n",
      "After 180 batches: [time:7.0m 2.4837892055511475s, Accuracy: 81.76795580110497]\n",
      "After 185 batches: [time:7.0m 14.767292261123657s, Accuracy: 81.58602150537635]\n",
      "After 190 batches: [time:7.0m 27.463687896728516s, Accuracy: 81.67539267015707]\n",
      "After 195 batches: [time:7.0m 38.83843398094177s, Accuracy: 81.69642857142857]\n",
      "After 200 batches: [time:7.0m 50.92911219596863s, Accuracy: 81.8407960199005]\n",
      "After 205 batches: [time:8.0m 3.230398654937744s, Accuracy: 81.85679611650485]\n",
      "After 210 batches: [time:8.0m 14.68586254119873s, Accuracy: 81.63507109004739]\n",
      "After 215 batches: [time:8.0m 25.863009691238403s, Accuracy: 81.6550925925926]\n",
      "After 220 batches: [time:8.0m 38.1362464427948s, Accuracy: 81.78733031674209]\n",
      "Training accuracy: 81.77777777777779\n",
      "Trainig evaluation time: 8.0m 48.066688537597656s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "t = time.time()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_labels = 0\n",
    "    correctly_pred = 0\n",
    "    t_batch = time.time()\n",
    "    for i, (data, labels) in enumerate(train_dataloader):\n",
    "#         data,labels = data.to(device), labels.to(device)\n",
    "        gc.collect()\n",
    "        X, y = data, labels\n",
    "        output = net(X)\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        total_labels += y.size(0)\n",
    "        correctly_pred += (y_pred==y).sum().item()\n",
    "        current_acc = (correctly_pred/total_labels)\n",
    "        if i%5==0:\n",
    "            seconds_passed = time.time()-t_batch\n",
    "            print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "    train_acc = (correctly_pred/total_labels)\n",
    "    print(f'Training accuracy: {train_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "\n",
    "print(f'Trainig evaluation time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 2.2839999198913574s, Accuracy: 87.5]\n",
      "After 5 batches: [time:0.0m 13.638270378112793s, Accuracy: 72.91666666666666]\n",
      "After 10 batches: [time:0.0m 24.395268440246582s, Accuracy: 77.27272727272727]\n",
      "After 15 batches: [time:0.0m 35.51371502876282s, Accuracy: 78.90625]\n",
      "After 20 batches: [time:0.0m 46.86592411994934s, Accuracy: 78.57142857142857]\n",
      "After 25 batches: [time:0.0m 58.227649211883545s, Accuracy: 78.36538461538461]\n",
      "After 30 batches: [time:1.0m 10.134474277496338s, Accuracy: 79.03225806451613]\n",
      "After 35 batches: [time:1.0m 22.719993114471436s, Accuracy: 79.16666666666666]\n",
      "Test accuracy: 79.33333333333333\n",
      "Test time: 1.0m 26.04461407661438s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "t = time.time()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "total_labels = 0\n",
    "correctly_pred = 0\n",
    "t_batch = time.time()\n",
    "test_batch=[]\n",
    "pred_batch=[]\n",
    "\n",
    "for i, (data, labels) in enumerate(test_dataloader):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    gc.collect()\n",
    "    X, y = data, labels\n",
    "    test_batch.extend(y.tolist())\n",
    "    output = net(X)\n",
    "    y_pred = torch.argmax(output, dim=1)\n",
    "    pred_batch.extend(y_pred.tolist())\n",
    "    total_labels += y.size(0)\n",
    "    correctly_pred += (y_pred==y).sum().item()\n",
    "    current_acc = (correctly_pred/total_labels)\n",
    "    if i%5==0:\n",
    "        seconds_passed = time.time()-t_batch\n",
    "        print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "\n",
    "test_acc = (correctly_pred/total_labels)\n",
    "print(f'Test accuracy: {test_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Test time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Saving pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "file_path = './temp/net.pt'\n",
    "\n",
    "state = {\n",
    "    'epoch': num_epoch,\n",
    "    'state_dict': net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'lr': lr\n",
    "}\n",
    "\n",
    "torch.save(state, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from torch.nn import Sequential, Module, Conv2d, MaxPool2d, Linear, ReLU, Dropout, BatchNorm2d, LeakyReLU, AdaptiveAvgPool2d, Flatten\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "import gc\n",
    "from PIL import Image\n",
    "from torchvision.datasets import VisionDataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io as sk_io, transform as sk_transform\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "file_path = './temp/net.pt'\n",
    "\n",
    "state = torch.load(file_path)\n",
    "\n",
    "loaded_net = ProjectModel()\n",
    "loaded_net.load_state_dict(state['state_dict'])\n",
    "\n",
    "loaded_lr = state['lr']\n",
    "loaded_loss_evaluater = CrossEntropyLoss()\n",
    "loaded_optimizer = Adam(loaded_net.parameters(), lr=loaded_lr)\n",
    "loaded_optimizer.load_state_dict(state['optimizer'])\n",
    "\n",
    "loaded_num_epoch = state['epoch']\n",
    "print('Model Loaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing loaded pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 batches: [time:0.0m 4.486730337142944s, Accuracy: 87.5]\n",
      "After 5 batches: [time:0.0m 56.63129281997681s, Accuracy: 72.91666666666666]\n",
      "After 10 batches: [time:1.0m 15.77672815322876s, Accuracy: 77.27272727272727]\n",
      "After 15 batches: [time:1.0m 27.282719373703003s, Accuracy: 78.90625]\n",
      "After 20 batches: [time:1.0m 38.31579399108887s, Accuracy: 78.57142857142857]\n",
      "After 25 batches: [time:1.0m 50.291762590408325s, Accuracy: 78.36538461538461]\n",
      "After 30 batches: [time:2.0m 2.352581262588501s, Accuracy: 79.03225806451613]\n",
      "After 35 batches: [time:2.0m 13.51013445854187s, Accuracy: 79.16666666666666]\n",
      "Test accuracy: 79.33333333333333\n",
      "Test time: 2.0m 17.30062699317932s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "loaded_net.eval()\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "total_labels = 0\n",
    "correctly_pred = 0\n",
    "t_batch = time.time()\n",
    "test_batch=[]\n",
    "pred_batch=[]\n",
    "\n",
    "for i, (data, labels) in enumerate(test_dataloader):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     data, labels = data.to(device), labels.to(device)\n",
    "    gc.collect()\n",
    "    X, y = data, labels\n",
    "    test_batch.extend(y.tolist())\n",
    "    output = loaded_net(X)\n",
    "    y_pred = torch.argmax(output, dim=1)\n",
    "    pred_batch.extend(y_pred.tolist())\n",
    "    total_labels += y.size(0)\n",
    "    correctly_pred += (y_pred==y).sum().item()\n",
    "    current_acc = (correctly_pred/total_labels)\n",
    "    if i%5==0:\n",
    "        seconds_passed = time.time()-t_batch\n",
    "        print(f'After {i} batches: [time:{seconds_passed//60}m {seconds_passed%60}s, Accuracy: {current_acc*100}]')\n",
    "\n",
    "test_acc = (correctly_pred/total_labels)\n",
    "print(f'Test accuracy: {test_acc*100}')\n",
    "seconds_passed = time.time()-t\n",
    "print(f'Test time: {seconds_passed//60}m {seconds_passed%60}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "mask_index=dataset.class_to_idx['mask']\n",
    "no_mask_index=dataset.class_to_idx['no_mask']\n",
    "not_person_index=dataset.class_to_idx['not_person']\n",
    "\n",
    "confusion_metrics_final=metrics.confusion_matrix(test_batch,pred_batch)\n",
    "precision_final=metrics.precision_score(test_batch, pred_batch, average=None)\n",
    "recall_final=metrics.recall_score(test_batch, pred_batch, average=None)\n",
    "f1_final=metrics.f1_score(test_batch, pred_batch, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the model is 79.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for the model is {test_acc*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for the class mask is 67.74193548387096 %\n",
      "Precision for the class no_mask is 84.72222222222221 %\n",
      "Precision for the class not_person is 89.42307692307693 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision for the class mask is {precision_final[mask_index]*100} %')\n",
    "print(f'Precision for the class no_mask is {precision_final[no_mask_index]*100} %')\n",
    "print(f'Precision for the class not_person is {precision_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for the class mask is 84.0 %\n",
      "Recall for the class no_mask is 61.0 %\n",
      "Recall for the class not_person is 93.0 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Recall for the class mask is {recall_final[mask_index]*100} %')\n",
    "print(f'Recall for the class no_mask is {recall_final[no_mask_index]*100} %')\n",
    "print(f'Recall for the class not_person is {recall_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for the class mask is 75.0 %\n",
      "F1 Score for the class no_mask is 70.93023255813952 %\n",
      "F1 Score for the class not_person is 91.1764705882353 %\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 Score for the class mask is {f1_final[mask_index]*100} %')\n",
    "print(f'F1 Score for the class no_mask is {f1_final[no_mask_index]*100} %')\n",
    "print(f'F1 Score for the class not_person is {f1_final[not_person_index]*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x26fa8067280>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "torch.manual_seed(6721)\n",
    "np.random.seed(6721)\n",
    "\n",
    "cm = confusion_matrix(test_batch, pred_batch, normalize='all')*100\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=['mask', 'no_mask', 'not_person'])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Confusion Matrix(%)')\n",
    "cmd.plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# == End =="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:comp6721]",
   "language": "python",
   "name": "conda-env-comp6721-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
